{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral Image Classification\n",
    "insert small description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from random import shuffle\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io as sio # Scipy input and output\n",
    "import scipy.ndimage \n",
    "from skimage.transform import rotate \n",
    "import spectral # Module for processing hyperspectral image data.\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "\n",
    "# scikit-learn imports \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "# keras imports \n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization, Dropout, Input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading  and Preproccesing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  load_dataset(dataset):\n",
    "    \"\"\"load dataset parameters from config.json\"\"\"\n",
    "    \n",
    "    with open('./config.json') as f:\n",
    "        config = json.loads(f.read())\n",
    "        params = config[dataset]\n",
    "        data = sio.loadmat(params['img_path'])[params['img']]\n",
    "        labels = sio.loadmat(params['gt_path'])[params['gt']]\n",
    "        num_classes = params['num_classes']\n",
    "        target_names = params['target_names']\n",
    "        \n",
    "    return data,labels,num_classes,target_names\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(X, num_components=75):\n",
    "    \"\"\"apply pca to X and return new_X\"\"\"\n",
    "    \n",
    "    new_X = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=num_components, whiten=True)\n",
    "    new_X = pca.fit_transform(new_X)\n",
    "    new_X = np.reshape(new_X, (X.shape[0],X.shape[1], num_components))\n",
    "    return new_X, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_with_zeros(X, margin=2):\n",
    "    \"\"\"apply zero padding to X with margin\"\"\"\n",
    "    \n",
    "    new_X = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    new_X[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return new_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patches(X, y, window_size=5, remove_zero_labels = True):\n",
    "    \"\"\"create patch from image. suppose the image has the shape (w,h,c) then the patch shape is\n",
    "    (w*h,window_size,window_size,c)\"\"\"\n",
    "    \n",
    "    margin = int((window_size - 1) / 2)\n",
    "    zero_padded_X = pad_with_zeros(X, margin=margin)\n",
    "    # split patches\n",
    "    patches_data = np.zeros((X.shape[0] * X.shape[1], window_size, window_size, X.shape[2]))\n",
    "    patchs_labels = np.zeros((X.shape[0] * X.shape[1]))\n",
    "    patch_index = 0\n",
    "    for r in range(margin, zero_padded_X.shape[0] - margin):\n",
    "        for c in range(margin, zero_padded_X.shape[1] - margin):\n",
    "            patch = zero_padded_X[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
    "            patches_data[patch_index, :, :, :] = patch\n",
    "            patchs_labels[patch_index] = y[r-margin, c-margin]\n",
    "            patch_index = patch_index + 1\n",
    "    if remove_zero_labels:\n",
    "        patches_data = patches_data[patchs_labels>0,:,:,:]\n",
    "        patchs_labels = patchs_labels[patchs_labels>0]\n",
    "        patchs_labels -= 1\n",
    "    return patches_data, patchs_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_set(X, y, test_ratio=0.10):\n",
    "    \"\"\"split dataset into train set and test set with test_ratio\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=345,\n",
    "                                                        stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_weak_classes(X, y):\n",
    "    \"\"\"\"balance the dataset by prforming oversample of weak classes (making each class have close labels_counts)\"\"\"\n",
    "    unique_labels, labels_counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "\n",
    "    max_count = np.max(labels_counts)\n",
    "    labels_inverse_ratios = max_count / labels_counts  \n",
    "    #print(labels_inverse_ratios)\n",
    "    # repeat for every label and concat\n",
    "    print(labels_inverse_ratios)\n",
    "    new_X = X[y == unique_labels[0], :, :, :].repeat(round(labels_inverse_ratios[0]), axis=0)\n",
    "    new_Y = y[y == unique_labels[0]].repeat(round(labels_inverse_ratios[0]), axis=0)\n",
    "    for label, labelInverseRatio in zip(unique_labels[1:], labels_inverse_ratios[1:]):\n",
    "        cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n",
    "        cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n",
    "        new_X = np.concatenate((new_X, cX))\n",
    "        new_Y = np.concatenate((new_Y, cY))\n",
    "    np.random.seed(seed=42)\n",
    "    rand_perm = np.random.permutation(new_Y.shape[0])\n",
    "    new_X = new_X[rand_perm, :, :, :]\n",
    "    new_Y = new_Y[rand_perm]\n",
    "    unique_labels, labels_counts = np.unique(new_Y, return_counts=True)\n",
    "    \n",
    "#     print(unique_labels.shape)\n",
    "#     print(unique_labels)\n",
    "#     print(labels_counts.shape)\n",
    "#     print(labels_counts)\n",
    "    return new_X, new_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X_train):\n",
    "    \"\"\"augment the data by taking each patch and randomly performing \n",
    "    a flip(up/down or right/left) or a rotation\"\"\"\n",
    "    \n",
    "    for i in range(int(X_train.shape[0]/2)):\n",
    "        patch = X_train[i,:,:,:]\n",
    "        num = random.randint(0,2)\n",
    "        if (num == 0):\n",
    "            \n",
    "            flipped_patch = np.flipud(patch)\n",
    "        if (num == 1):\n",
    "            \n",
    "            flipped_patch = np.fliplr(patch)\n",
    "        if (num == 2):\n",
    "            \n",
    "            no = random.randrange(-180,180,30)\n",
    "            flipped_patch = scipy.ndimage.interpolation.rotate(patch, no,axes=(1, 0),\n",
    "                                                               reshape=False, output=None, order=3, mode='constant', cval=0.0, prefilter=False)\n",
    "    \n",
    "    \n",
    "        patch2 = flipped_patch\n",
    "        X_train[i,:,:,:] = patch2\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "dataset = \"Indian_pines\" # Indian_pines or PaviaU or or Salinas  . check config.json\n",
    "window_size = 25\n",
    "num_pca_components = 30\n",
    "test_ratio = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproccesing steps\n",
    "<!---img src=\"./images/1.png\" alt=\"diagram\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial (145, 145, 200)\n",
      "PCA (145, 145, 30)\n",
      "Patches (10249, 25, 25, 30) \n",
      "Split (3074, 25, 25, 30)\n"
     ]
    }
   ],
   "source": [
    "X, y , num_classes , target_names = load_dataset(dataset)\n",
    "print(\"Initial {}\".format(X.shape))\n",
    "\n",
    "X,pca = apply_pca(X,num_pca_components)\n",
    "print(\"PCA {}\".format(X.shape))\n",
    "\n",
    "X_patches, y_patches = create_patches(X, y, window_size=window_size)\n",
    "print(\"Patches {} \".format(X_patches.shape))\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_train_test_set(X_patches, y_patches, test_ratio)\n",
    "print(\"Split {}\".format(X_train.shape))\n",
    "\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train) # convert class labels to on-hot encoding\n",
    "y_test = np_utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3074, 25, 25, 30, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1,window_size,window_size,num_pca_components,1)\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 25, 25, 30, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 23, 23, 24, 8)     512       \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 21, 21, 20, 16)    5776      \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 19, 19, 18, 32)    13856     \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 19, 19, 576)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 17, 17, 64)        331840    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               4735232   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 5,122,176\n",
      "Trainable params: 5,122,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## input layer\n",
    "input_layer = Input((window_size, window_size, num_pca_components,1))\n",
    "\n",
    "## convolutional layers\n",
    "conv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 7), activation='relu')(input_layer)\n",
    "conv_layer2 = Conv3D(filters=16,kernel_size=(3,3,5),activation='relu')(conv_layer1)\n",
    "conv_layer3 = Conv3D(filters=32,kernel_size=(3,3,3),activation='relu')(conv_layer2)\n",
    "\n",
    "conv3d_shape = conv_layer3._keras_shape\n",
    "conv_layer3 = Reshape((conv3d_shape[1],conv3d_shape[2],conv3d_shape[3]*conv3d_shape[4]))(conv_layer3)\n",
    "conv_layer4 = Conv2D(filters=64,kernel_size=(3,3),activation='relu')(conv_layer3)\n",
    "\n",
    "flatten_layer = Flatten()(conv_layer4)\n",
    "\n",
    "## fully connected layers\n",
    "dense_layer1 = Dense(units=256,activation='relu')(flatten_layer)\n",
    "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
    "dense_layer2 = Dense(units=128,activation='relu')(dense_layer1)\n",
    "dense_layer2 = Dropout(0.4)(dense_layer2)\n",
    "\n",
    "output_layer = Dense(units=num_classes,activation='softmax')(dense_layer2)\n",
    "    \n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.summary()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.001,decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=256, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./saved_models/model_hybrid_{}_{}_{}_{}.h5'.format(dataset,window_size,num_pca_components,test_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./saved_models/model_{}_{}_{}_{}.h5'.format(dataset,window_size,num_pca_components,test_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report (X_test,y_test):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    \n",
    "    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n",
    "    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "    score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "    test_Loss =  score[0]*100\n",
    "    test_accuracy = score[1]*100\n",
    "    \n",
    "    return classification, confusion, test_Loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_report(classification, confusion, test_loss, test_accuracy) :\n",
    "    \n",
    "    classification = str(classification)\n",
    "    confusion = str(confusion)\n",
    "\n",
    "    report = '{} Test loss (%)'.format(test_loss)\n",
    "    report += '\\n'\n",
    "    report += '{} Test accuracy (%)'.format(test_accuracy)\n",
    "    report += '\\n'\n",
    "    report += '\\n'\n",
    "    report += '{}'.format(classification)\n",
    "    report += '\\n'\n",
    "    report += '{}'.format(confusion)\n",
    "    print(report)\n",
    "    \n",
    "    return report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report(report):\n",
    "    file_name = 'report_{}_{}_{}_{}.txt'.format(dataset,window_size,num_pca_components,test_ratio)\n",
    "    with open('./reports/{}'.format(file_name), 'w') as report_file:\n",
    "        report_file.write(report)\n",
    "        print(\"\\n\\nReport saved to {}\".format(file_name))\n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch(data,height_index,width_index):\n",
    "    #transpose_array = data.transpose((2,0,1))\n",
    "    #print transpose_array.shape\n",
    "    height_slice = slice(height_index, height_index+patch_size)\n",
    "    width_slice = slice(width_index, width_index+patch_size)\n",
    "    patch = data[height_slice, width_slice, :]\n",
    "    \n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the pretrained model make predictions and print the results into a reportclassification, confusion, test_loss, test_accuracy = generate_report(X_test,y_test)\n",
    "\n",
    "classification, confusion, test_Loss, test_accuracy = generate_report(X_test,y_test)\n",
    "report = show_report(classification, confusion, test_Loss, test_accuracy)\n",
    "save_report(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classification Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original image\n",
    "dataset=\"Indian_pines\"\n",
    "X, y , num_classes,target_names= load_dataset(dataset)\n",
    "X,pca = apply_pca(X,num_pca_components)\n",
    "height = y.shape[0]\n",
    "width = y.shape[1]\n",
    "patch_size = window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pridected_image():\n",
    "    \"\"\"generate the predicted image\"\"\"\n",
    "    outputs = np.zeros((height,width))\n",
    "\n",
    "    for i in range(height-patch_size+1):\n",
    "        for j in range(width-patch_size+1):\n",
    "            target = y[int(i+patch_size/2), int(j+patch_size/2)]\n",
    "            if target == 0 :\n",
    "                continue\n",
    "            else :\n",
    "                image_patch=get_patch(X,i,j)\n",
    "                #print (image_patch.shape)\n",
    "                X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1],image_patch.shape[2]).astype('float32')                                   \n",
    "                prediction = (model.predict_classes(X_test_image))                         \n",
    "                outputs[int(i+patch_size/2)][int(j+patch_size/2)] = prediction+1\n",
    "    return outputs.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Ground Truth Image\n",
    "ground_truth = spectral.imshow(classes = y,figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the Predicted image\n",
    "outputs=generate_pridected_image()\n",
    "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython3",
   "language": "python",
   "name": "mypython3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

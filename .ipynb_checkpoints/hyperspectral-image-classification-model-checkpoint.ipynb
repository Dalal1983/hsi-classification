{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral Image Classification\n",
    "insert small description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from random import shuffle\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io as sio # Scipy input and output\n",
    "import scipy.ndimage \n",
    "from skimage.transform import rotate \n",
    "import spectral # Module for processing hyperspectral image data.\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "\n",
    "# scikit-learn imports \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# keras imports \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading  and Preproccesing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  load_dataset(dataset):\n",
    "    \"\"\"load dataset parameters from config.json\"\"\"\n",
    "    \n",
    "    with open('./config.json') as f:\n",
    "        config = json.loads(f.read())\n",
    "        params = config[dataset]\n",
    "        data = sio.loadmat(params['img_path'])[params['img']]\n",
    "        labels = sio.loadmat(params['gt_path'])[params['gt']]\n",
    "        num_classes = params['num_classes']\n",
    "        target_names = params['target_names']\n",
    "        \n",
    "    return data,labels,num_classes,target_names\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(X, num_components=75):\n",
    "    \"\"\"apply pca to X and return new_X\"\"\"\n",
    "    \n",
    "    new_X = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=num_components, whiten=True)\n",
    "    new_X = pca.fit_transform(new_X)\n",
    "    new_X = np.reshape(new_X, (X.shape[0],X.shape[1], num_components))\n",
    "    return new_X, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_with_zeros(X, margin=2):\n",
    "    \"\"\"apply zero padding to X with margin\"\"\"\n",
    "    \n",
    "    new_X = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    new_X[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return new_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patches(X, y, window_size=5, remove_zero_labels = True):\n",
    "    \"\"\"create patch from image. suppose the image has the shape (w,h,c) then the patch shape is\n",
    "    (w*h,window_size,window_size,c)\"\"\"\n",
    "    \n",
    "    margin = int((window_size - 1) / 2)\n",
    "    zero_padded_X = pad_with_zeros(X, margin=margin)\n",
    "    # split patches\n",
    "    patches_data = np.zeros((X.shape[0] * X.shape[1], window_size, window_size, X.shape[2]))\n",
    "    patchs_labels = np.zeros((X.shape[0] * X.shape[1]))\n",
    "    patch_index = 0\n",
    "    for r in range(margin, zero_padded_X.shape[0] - margin):\n",
    "        for c in range(margin, zero_padded_X.shape[1] - margin):\n",
    "            patch = zero_padded_X[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
    "            patches_data[patch_index, :, :, :] = patch\n",
    "            patchs_labels[patch_index] = y[r-margin, c-margin]\n",
    "            patch_index = patch_index + 1\n",
    "    if remove_zero_labels:\n",
    "        patches_data = patches_data[patchs_labels>0,:,:,:]\n",
    "        patchs_labels = patchs_labels[patchs_labels>0]\n",
    "        patchs_labels -= 1\n",
    "    return patches_data, patchs_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_set(X, y, test_ratio=0.10):\n",
    "    \"\"\"split dataset into train set and test set with test_ratio\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=345,\n",
    "                                                        stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_weak_classes(X, y):\n",
    "    \"\"\"\"balance the dataset by prforming oversample of weak classes (making each class have close labels_counts)\"\"\"\n",
    "    unique_labels, labels_counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "#     print(unique_labels.shape)\n",
    "#     print(unique_labels)\n",
    "#     print(labels_counts.shape)\n",
    "#     print(labels_counts)\n",
    "\n",
    "    max_count = np.max(labels_counts)\n",
    "    labels_inverse_ratios = max_count / labels_counts  \n",
    "    #print(labels_inverse_ratios)\n",
    "    # repeat for every label and concat\n",
    "    new_X = X[y == unique_labels[0], :, :, :].repeat(round(labels_inverse_ratios[0]), axis=0)\n",
    "    new_Y = y[y == unique_labels[0]].repeat(round(labels_inverse_ratios[0]), axis=0)\n",
    "    for label, labelInverseRatio in zip(unique_labels[1:], labels_inverse_ratios[1:]):\n",
    "        cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n",
    "        cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n",
    "        new_X = np.concatenate((new_X, cX))\n",
    "        new_Y = np.concatenate((new_Y, cY))\n",
    "    np.random.seed(seed=42)\n",
    "    rand_perm = np.random.permutation(new_Y.shape[0])\n",
    "    new_X = new_X[rand_perm, :, :, :]\n",
    "    new_Y = new_Y[rand_perm]\n",
    "    unique_labels, labels_counts = np.unique(new_Y, return_counts=True)\n",
    "    \n",
    "#     print(unique_labels.shape)\n",
    "#     print(unique_labels)\n",
    "#     print(labels_counts.shape)\n",
    "#     print(labels_counts)\n",
    "    return new_X, new_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X_train):\n",
    "    \"\"\"augment the data by taking each patch and randomly performing \n",
    "    a flip(up/down or right/left) or a rotation\"\"\"\n",
    "    \n",
    "    for i in range(int(X_train.shape[0]/2)):\n",
    "        patch = X_train[i,:,:,:]\n",
    "        num = random.randint(0,2)\n",
    "        if (num == 0):\n",
    "            \n",
    "            flipped_patch = np.flipud(patch)\n",
    "        if (num == 1):\n",
    "            \n",
    "            flipped_patch = np.fliplr(patch)\n",
    "        if (num == 2):\n",
    "            \n",
    "            no = random.randrange(-180,180,30)\n",
    "            flipped_patch = scipy.ndimage.interpolation.rotate(patch, no,axes=(1, 0),\n",
    "                                                               reshape=False, output=None, order=3, mode='constant', cval=0.0, prefilter=False)\n",
    "    \n",
    "    \n",
    "        patch2 = flipped_patch\n",
    "        X_train[i,:,:,:] = patch2\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "dataset = \"Salinas\" # Indian_pines or PaviaU or or Salinas  . check config.json\n",
    "window_size = 5\n",
    "num_pca_components = 30\n",
    "test_ratio = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproccesing steps\n",
    "<img src=\"./images/1.png\" alt=\"diagram\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial (512, 217, 204)\n",
      "PCA (512, 217, 30)\n",
      "Patches (54129, 5, 5, 30) \n",
      "(54129,)\n",
      "Split (40596, 5, 5, 30)\n",
      "Oversample (138400, 5, 5, 30)\n",
      "Augmentaion (138400, 5, 5, 30) \n"
     ]
    }
   ],
   "source": [
    "X, y , num_classes , target_names = load_dataset(dataset)\n",
    "print(\"Initial {}\".format(X.shape))\n",
    "X,pca = apply_pca(X,num_pca_components)\n",
    "print(\"PCA {}\".format(X.shape))\n",
    "X_patches, y_patches = create_patches(X, y, window_size=window_size)\n",
    "print(\"Patches {} \".format(X_patches.shape))\n",
    "print(y_patches.shape)\n",
    "X_train, X_test, y_train, y_test = split_train_test_set(X_patches, y_patches, test_ratio)\n",
    "print(\"Split {}\".format(X_train.shape))\n",
    "X_train, y_train = oversample_weak_classes(X_train, y_train)\n",
    "print(\"Oversample {}\".format(X_train.shape))\n",
    "X_train = augment_data(X_train)\n",
    "print(\"Augmentaion {} \".format(X_train.shape))\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train) # convert class labels to on-hot encoding\n",
    "y_test = np_utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model() :\n",
    "    \n",
    "    \n",
    "    input_shape= X_train[0].shape # Define the input shape \n",
    "    C1 = 3*num_pca_components # number of filters\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(C1, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(3*C1, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(6*num_pca_components, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 90, 3, 28)         4140      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 270, 1, 26)        218970    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 270, 1, 26)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7020)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 180)               1263780   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                2896      \n",
      "=================================================================\n",
      "Total params: 1,489,786\n",
      "Trainable params: 1,489,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ali/anaconda2/envs/mypython3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/15\n",
      "138400/138400 [==============================] - 244s 2ms/step - loss: 0.6000 - acc: 0.8471\n",
      "Epoch 2/15\n",
      "138400/138400 [==============================] - 237s 2ms/step - loss: 0.0975 - acc: 0.9707\n",
      "Epoch 3/15\n",
      "138400/138400 [==============================] - 242s 2ms/step - loss: 0.0657 - acc: 0.9795\n",
      "Epoch 4/15\n",
      "138400/138400 [==============================] - 241s 2ms/step - loss: 0.0543 - acc: 0.9825\n",
      "Epoch 5/15\n",
      "138400/138400 [==============================] - 242s 2ms/step - loss: 0.0468 - acc: 0.9844\n",
      "Epoch 6/15\n",
      "138400/138400 [==============================] - 241s 2ms/step - loss: 0.0408 - acc: 0.9866\n",
      "Epoch 7/15\n",
      "138400/138400 [==============================] - 254s 2ms/step - loss: 0.0373 - acc: 0.9877\n",
      "Epoch 8/15\n",
      "  1984/138400 [..............................] - ETA: 4:09 - loss: 0.0312 - acc: 0.9889"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./saved_models/model1_{}_{}_{}_{}.h5'.format(dataset,window_size,num_pca_components,test_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./saved_models/model_{}_{}_{}_{}.h5'.format(dataset,window_size,num_pca_components,test_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report (X_test,y_test):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    \n",
    "    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n",
    "    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "    score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "    test_Loss =  score[0]*100\n",
    "    test_accuracy = score[1]*100\n",
    "    \n",
    "    return classification, confusion, test_Loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_report(classification, confusion, test_loss, test_accuracy) :\n",
    "    \n",
    "    classification = str(classification)\n",
    "    confusion = str(confusion)\n",
    "\n",
    "    report = '{} Test loss (%)'.format(test_loss)\n",
    "    report += '\\n'\n",
    "    report += '{} Test accuracy (%)'.format(test_accuracy)\n",
    "    report += '\\n'\n",
    "    report += '\\n'\n",
    "    report += '{}'.format(classification)\n",
    "    report += '\\n'\n",
    "    report += '{}'.format(confusion)\n",
    "    print(report)\n",
    "    \n",
    "    return report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report(report):\n",
    "    file_name = 'report_{}_{}_{}_{}.txt'.format(dataset,window_size,num_pca_components,test_ratio)\n",
    "    with open('./reports/{}'.format(file_name), 'w') as report_file:\n",
    "        report_file.write(report)\n",
    "        print(\"\\n\\nReport saved to {}\".format(file_name))\n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch(data,height_index,width_index):\n",
    "    #transpose_array = data.transpose((2,0,1))\n",
    "    #print transpose_array.shape\n",
    "    height_slice = slice(height_index, height_index+patch_size)\n",
    "    width_slice = slice(width_index, width_index+patch_size)\n",
    "    patch = data[height_slice, width_slice, :]\n",
    "    \n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the pretrained model make predictions and print the results into a reportclassification, confusion, test_loss, test_accuracy = generate_report(X_test,y_test)\n",
    "\n",
    "classification, confusion, test_Loss, test_accuracy = generate_report(X_test,y_test)\n",
    "report = show_report(classification, confusion, test_Loss, test_accuracy)\n",
    "save_report(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classification Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original image\n",
    "dataset=\"Indian_pines\"\n",
    "X, y , num_classes,target_names= load_dataset(dataset)\n",
    "X,pca = apply_pca(X,num_pca_components)\n",
    "height = y.shape[0]\n",
    "width = y.shape[1]\n",
    "patch_size = window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pridected_image():\n",
    "    \"\"\"generate the predicted image\"\"\"\n",
    "    outputs = np.zeros((height,width))\n",
    "\n",
    "    for i in range(height-patch_size+1):\n",
    "        for j in range(width-patch_size+1):\n",
    "            target = y[int(i+patch_size/2), int(j+patch_size/2)]\n",
    "            if target == 0 :\n",
    "                continue\n",
    "            else :\n",
    "                image_patch=get_patch(X,i,j)\n",
    "                #print (image_patch.shape)\n",
    "                X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1],image_patch.shape[2]).astype('float32')                                   \n",
    "                prediction = (model.predict_classes(X_test_image))                         \n",
    "                outputs[int(i+patch_size/2)][int(j+patch_size/2)] = prediction+1\n",
    "    return outputs.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Ground Truth Image\n",
    "ground_truth = spectral.imshow(classes = y,figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the Predicted image\n",
    "outputs=generate_pridected_image()\n",
    "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython3",
   "language": "python",
   "name": "mypython3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

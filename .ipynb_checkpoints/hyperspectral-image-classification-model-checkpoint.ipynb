{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral Image Classification\n",
    "insert small description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from random import shuffle\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io as sio # Scipy input and output\n",
    "import scipy.ndimage \n",
    "from skimage.transform import rotate \n",
    "import spectral # Module for processing hyperspectral image data.\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "\n",
    "# scikit-learn imports \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "# keras imports \n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization, Dropout, Input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading  and Preproccesing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  load_dataset(dataset):\n",
    "    \"\"\"load dataset parameters from config.json\"\"\"\n",
    "    \n",
    "    with open('./config.json') as f:\n",
    "        config = json.loads(f.read())\n",
    "        params = config[dataset]\n",
    "        data = sio.loadmat(params['img_path'])[params['img']]\n",
    "        labels = sio.loadmat(params['gt_path'])[params['gt']]\n",
    "        num_classes = params['num_classes']\n",
    "        target_names = params['target_names']\n",
    "        \n",
    "    return data,labels,num_classes,target_names\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(X, num_components=75):\n",
    "    \"\"\"apply pca to X and return new_X\"\"\"\n",
    "    \n",
    "    new_X = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=num_components, whiten=True)\n",
    "    new_X = pca.fit_transform(new_X)\n",
    "    new_X = np.reshape(new_X, (X.shape[0],X.shape[1], num_components))\n",
    "    return new_X, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_with_zeros(X, margin=2):\n",
    "    \"\"\"apply zero padding to X with margin\"\"\"\n",
    "    \n",
    "    new_X = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    new_X[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return new_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patches(X, y, window_size=5, remove_zero_labels = True):\n",
    "    \"\"\"create patch from image. suppose the image has the shape (w,h,c) then the patch shape is\n",
    "    (w*h,window_size,window_size,c)\"\"\"\n",
    "    \n",
    "    margin = int((window_size - 1) / 2)\n",
    "    zero_padded_X = pad_with_zeros(X, margin=margin)\n",
    "    # split patches\n",
    "    patches_data = np.zeros((X.shape[0] * X.shape[1], window_size, window_size, X.shape[2]))\n",
    "    patchs_labels = np.zeros((X.shape[0] * X.shape[1]))\n",
    "    patch_index = 0\n",
    "    for r in range(margin, zero_padded_X.shape[0] - margin):\n",
    "        for c in range(margin, zero_padded_X.shape[1] - margin):\n",
    "            patch = zero_padded_X[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
    "            patches_data[patch_index, :, :, :] = patch\n",
    "            patchs_labels[patch_index] = y[r-margin, c-margin]\n",
    "            patch_index = patch_index + 1\n",
    "    if remove_zero_labels:\n",
    "        patches_data = patches_data[patchs_labels>0,:,:,:]\n",
    "        patchs_labels = patchs_labels[patchs_labels>0]\n",
    "        patchs_labels -= 1\n",
    "    return patches_data, patchs_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_set(X, y, test_ratio=0.10):\n",
    "    \"\"\"split dataset into train set and test set with test_ratio\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=345,\n",
    "                                                        stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_weak_classes(X, y):\n",
    "    \"\"\"\"balance the dataset by prforming oversample of weak classes (making each class have close labels_counts)\"\"\"\n",
    "    unique_labels, labels_counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "\n",
    "    max_count = np.max(labels_counts)\n",
    "    labels_inverse_ratios = max_count / labels_counts  \n",
    "    #print(labels_inverse_ratios)\n",
    "    # repeat for every label and concat\n",
    "    #print(labels_inverse_ratios)\n",
    "    new_X = X[y == unique_labels[0], :, :, :].repeat(round(labels_inverse_ratios[0]), axis=0)\n",
    "    new_Y = y[y == unique_labels[0]].repeat(round(labels_inverse_ratios[0]), axis=0)\n",
    "    for label, labelInverseRatio in zip(unique_labels[1:], labels_inverse_ratios[1:]):\n",
    "        cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n",
    "        cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n",
    "        new_X = np.concatenate((new_X, cX))\n",
    "        new_Y = np.concatenate((new_Y, cY))\n",
    "    np.random.seed(seed=42)\n",
    "    rand_perm = np.random.permutation(new_Y.shape[0])\n",
    "    new_X = new_X[rand_perm, :, :, :]\n",
    "    new_Y = new_Y[rand_perm]\n",
    "    unique_labels, labels_counts = np.unique(new_Y, return_counts=True)\n",
    "    \n",
    "#     print(unique_labels.shape)\n",
    "#     print(unique_labels)\n",
    "#     print(labels_counts.shape)\n",
    "#     print(labels_counts)\n",
    "    return new_X, new_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X_train):\n",
    "    \"\"\"augment the data by taking each patch and randomly performing \n",
    "    a flip(up/down or right/left) or a rotation\"\"\"\n",
    "    \n",
    "    for i in range(int(X_train.shape[0]/2)):\n",
    "        patch = X_train[i,:,:,:]\n",
    "        num = random.randint(0,2)\n",
    "        if (num == 0):\n",
    "            \n",
    "            flipped_patch = np.flipud(patch)\n",
    "        if (num == 1):\n",
    "            \n",
    "            flipped_patch = np.fliplr(patch)\n",
    "        if (num == 2):\n",
    "            \n",
    "            no = random.randrange(-180,180,30)\n",
    "            flipped_patch = scipy.ndimage.interpolation.rotate(patch, no,axes=(1, 0),\n",
    "                                                               reshape=False, output=None, order=3, mode='constant', cval=0.0, prefilter=False)\n",
    "    \n",
    "    \n",
    "        patch2 = flipped_patch\n",
    "        X_train[i,:,:,:] = patch2\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "dataset = \"SalinasSh\" # Indian_pines or PaviaU or or Salinas  . check config.json\n",
    "window_size = 15\n",
    "num_pca_components = 3\n",
    "test_ratio = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproccesing steps\n",
    "<!---img src=\"./images/1.png\" alt=\"diagram\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial (512, 217, 3)\n",
      "Patches (54129, 15, 15, 3) \n",
      "Split (16238, 15, 15, 3)\n"
     ]
    }
   ],
   "source": [
    "X, y , num_classes , target_names = load_dataset(dataset)\n",
    "print(\"Initial {}\".format(X.shape))\n",
    "\n",
    "# X,pca = apply_pca(X,num_pca_components)\n",
    "# print(\"PCA {}\".format(X.shape))\n",
    "\n",
    "X_patches, y_patches = create_patches(X, y, window_size=window_size)\n",
    "print(\"Patches {} \".format(X_patches.shape))\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_train_test_set(X_patches, y_patches, test_ratio)\n",
    "print(\"Split {}\".format(X_train.shape))\n",
    "\n",
    "# X_train, y_train = oversample_weak_classes(X_train,y_train)\n",
    "# print(\"Oversampling {}\".format(X_train.shape))\n",
    "\n",
    "# X_train = augment_data(X_train)\n",
    "# print(\"Augmentation {}\".format(X_train.shape))\n",
    "\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train) # convert class labels to on-hot encoding\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16238, 15, 15, 3, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1,window_size,window_size,3,1)\n",
    "X_test = X_test.reshape(-1,window_size,window_size,3,1)\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 15, 15, 3, 1)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 13, 13, 1, 8)      224       \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 32)          4640      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2592)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               663808    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 673,952\n",
      "Trainable params: 673,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## input layer\n",
    "input_layer = Input((window_size, window_size, 3,1))\n",
    "\n",
    "## convolutional layers\n",
    "conv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
    "\n",
    "conv3d_shape = conv_layer1._keras_shape\n",
    "conv_layer1 = Reshape((conv3d_shape[1],conv3d_shape[2],conv3d_shape[3]*conv3d_shape[4]))(conv_layer1)\n",
    "\n",
    "\n",
    "conv_layer2 = Conv2D(filters=16,kernel_size=(3,3) ,activation='relu')(conv_layer1)\n",
    "conv_layer3 = Conv2D(filters=32,kernel_size=(3,3),activation='relu')(conv_layer2)\n",
    "\n",
    "flatten_layer = Flatten()(conv_layer3)\n",
    "\n",
    "## fully connected layers\n",
    "dense_layer1 = Dense(units=256,activation='relu')(flatten_layer)\n",
    "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
    "# dense_layer2 = Dense(units=128,activation='relu')(dense_layer1)\n",
    "# dense_layer2 = Dropout(0.4)(dense_layer2)\n",
    "\n",
    "output_layer = Dense(units=num_classes,activation='softmax')(dense_layer1)\n",
    "    \n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.summary()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.001,decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16238/16238 [==============================] - 6s 347us/step - loss: 0.2780 - accuracy: 0.8898\n",
      "Epoch 2/10\n",
      "16238/16238 [==============================] - 6s 354us/step - loss: 0.2225 - accuracy: 0.9084\n",
      "Epoch 3/10\n",
      "16238/16238 [==============================] - 5s 329us/step - loss: 0.2231 - accuracy: 0.9186\n",
      "Epoch 4/10\n",
      "16238/16238 [==============================] - 5s 333us/step - loss: 0.2521 - accuracy: 0.9001\n",
      "Epoch 5/10\n",
      "16238/16238 [==============================] - 6s 344us/step - loss: 0.2276 - accuracy: 0.9103\n",
      "Epoch 6/10\n",
      "16238/16238 [==============================] - 6s 344us/step - loss: 0.2303 - accuracy: 0.9084\n",
      "Epoch 7/10\n",
      "16238/16238 [==============================] - 5s 336us/step - loss: 0.2328 - accuracy: 0.9084\n",
      "Epoch 8/10\n",
      "16238/16238 [==============================] - 6s 374us/step - loss: 0.2733 - accuracy: 0.8898\n",
      "Epoch 9/10\n",
      "16238/16238 [==============================] - 5s 334us/step - loss: 0.2207 - accuracy: 0.9170\n",
      "Epoch 10/10\n",
      "16238/16238 [==============================] - 5s 329us/step - loss: 0.2069 - accuracy: 0.9202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f2200b18f50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=256, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./saved_models/lab/model_hybrid_{}_{}_{}_{}.h5'.format(dataset,window_size,num_pca_components,test_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./saved_models/lab/model_hybrid_{}_{}_{}_{}.h5'.format(dataset,window_size,num_pca_components,test_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report (X_test,y_test):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    \n",
    "    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n",
    "    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "    score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "    test_Loss =  score[0]*100\n",
    "    test_accuracy = score[1]*100\n",
    "    \n",
    "    return classification, confusion, test_Loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_report(classification, confusion, test_loss, test_accuracy) :\n",
    "    \n",
    "    classification = str(classification)\n",
    "    confusion = str(confusion)\n",
    "\n",
    "    report = '{} Test loss (%)'.format(test_loss)\n",
    "    report += '\\n'\n",
    "    report += '{} Test accuracy (%)'.format(test_accuracy)\n",
    "    report += '\\n'\n",
    "    report += '\\n'\n",
    "    report += '{}'.format(classification)\n",
    "    report += '\\n'\n",
    "    report += '{}'.format(confusion)\n",
    "    print(report)\n",
    "    \n",
    "    return report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report(report):\n",
    "    file_name = 'report_hybrid_{}_{}_{}_{}.txt'.format(dataset,window_size,num_pca_components,test_ratio)\n",
    "    with open('./reports/{}'.format(file_name), 'w') as report_file:\n",
    "        report_file.write(report)\n",
    "        print(\"\\n\\nReport saved to {}\".format(file_name))\n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch(data,height_index,width_index):\n",
    "    #transpose_array = data.transpose((2,0,1))\n",
    "    #print transpose_array.shape\n",
    "    height_slice = slice(height_index, height_index+patch_size)\n",
    "    width_slice = slice(width_index, width_index+patch_size)\n",
    "    patch = data[height_slice, width_slice, :]\n",
    "    \n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7175/7175 [==============================] - 15s 2ms/step\n",
      "3.041895230915657 Test loss (%)\n",
      "99.19163584709167 Test accuracy (%)\n",
      "\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "                     Alfalfa       1.00      0.94      0.97        32\n",
      "                 Corn-notill       0.98      1.00      0.99      1000\n",
      "                Corn-mintill       0.98      1.00      0.99       581\n",
      "                        Corn       1.00      0.99      0.99       166\n",
      "               Grass-pasture       0.99      0.99      0.99       338\n",
      "                 Grass-trees       0.99      1.00      0.99       511\n",
      "         Grass-pasture-mowed       1.00      0.90      0.95        20\n",
      "               Hay-windrowed       0.99      1.00      1.00       335\n",
      "                        Oats       1.00      0.57      0.73        14\n",
      "              Soybean-notill       1.00      0.98      0.99       680\n",
      "             Soybean-mintill       1.00      0.99      0.99      1719\n",
      "               Soybean-clean       1.00      0.98      0.99       415\n",
      "                       Wheat       1.00      0.99      1.00       143\n",
      "                       Woods       1.00      1.00      1.00       886\n",
      "Buildings-Grass-Trees-Drives       0.99      0.99      0.99       270\n",
      "          Stone-Steel-Towers       0.98      1.00      0.99        65\n",
      "\n",
      "                    accuracy                           0.99      7175\n",
      "                   macro avg       0.99      0.96      0.97      7175\n",
      "                weighted avg       0.99      0.99      0.99      7175\n",
      "\n",
      "[[  30    0    0    0    0    0    0    1    0    1    0    0    0    0\n",
      "     0    0]\n",
      " [   0  999    0    0    0    0    0    0    0    0    1    0    0    0\n",
      "     0    0]\n",
      " [   0    0  580    0    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    2    0  164    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0  336    0    0    0    0    0    1    0    0    0\n",
      "     1    0]\n",
      " [   0    0    0    0    0  510    0    0    0    0    0    0    0    0\n",
      "     1    0]\n",
      " [   0    0    0    0    0    0   18    1    0    0    0    0    0    0\n",
      "     1    0]\n",
      " [   0    0    0    0    0    0    0  335    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    5    0    1    0    0    0    8    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0   11    1    0    0    0    0    0    0  668    0    0    0    0\n",
      "     0    0]\n",
      " [   0    9    0    0    0    5    0    0    0    1 1703    0    0    1\n",
      "     0    0]\n",
      " [   0    3    5    0    0    0    0    0    0    0    0  406    0    0\n",
      "     0    1]\n",
      " [   0    0    1    0    0    0    0    0    0    0    0    0  142    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    1    0    0    0    0    0  885\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    2    0    0\n",
      "   268    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0   65]]\n",
      "\n",
      "\n",
      "Report saved to report_hybrid_Indian_pines_15_30_0.7.txt\n"
     ]
    }
   ],
   "source": [
    "# Using the pretrained model make predictions and print the results into a reportclassification, confusion, test_loss, test_accuracy = generate_report(X_test,y_test)\n",
    "\n",
    "classification, confusion, test_Loss, test_accuracy = generate_report(X_test,y_test)\n",
    "report = show_report(classification, confusion, test_Loss, test_accuracy)\n",
    "save_report(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classification Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original image\n",
    "dataset=\"Salinas\"\n",
    "X, y , num_classes,target_names= load_dataset(dataset)\n",
    "X,pca = apply_pca(X,num_pca_components)\n",
    "height = y.shape[0]\n",
    "width = y.shape[1]\n",
    "patch_size = window_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pridected_image():\n",
    "    \"\"\"generate the predicted image\"\"\"\n",
    "    outputs = np.zeros((height,width))\n",
    "\n",
    "    for i in range(height-patch_size+1):\n",
    "        for j in range(width-patch_size+1):\n",
    "            target = y[int(i+patch_size/2), int(j+patch_size/2)]\n",
    "            if target == 0 :\n",
    "                continue\n",
    "            else :\n",
    "                image_patch=get_patch(X,i,j)\n",
    "                #print (image_patch.shape)\n",
    "                X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1],image_patch.shape[2]).astype('float32')                                   \n",
    "                X_test_image = X_test_image.reshape(-1,window_size,window_size,num_pca_components,1)\n",
    "                prediction = (model.predict(X_test_image))\n",
    "                prediction= np.argmax(prediction,axis=1)\n",
    "                outputs[int(i+patch_size/2)][int(j+patch_size/2)] = prediction+1\n",
    "    return outputs.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the Ground Truth Image\n",
    "ground_truth = spectral.imshow(classes = y,figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the Predicted image\n",
    "outputs=generate_pridected_image()\n",
    "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
